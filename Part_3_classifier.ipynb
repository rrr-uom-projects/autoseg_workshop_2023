{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier to select the appropriate segmentation model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following cells are used to load data into the session. You shouldn't have to edit them, just make sure they work!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import getFiles\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import random\n",
    "import google.colab\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some of the abdominal and HnN data from the directories already created.\n",
    "\n",
    "# setup paths\n",
    "abdo_datapath = \"/content/FLARE_data/train/ims/\"\n",
    "hn_datapath = \"/content/HnN_data/train/ims/\"\n",
    "abdo_datapath = \"./FLARE_data/\"\n",
    "hn_datapath = \"./HnN_data/\"\n",
    "\n",
    "# get fnames\n",
    "abdo_fnames = sorted(getFiles(abdo_datapath))\n",
    "hn_fnames = sorted(getFiles(hn_datapath))\n",
    "\n",
    "# sample 400 images of each dataset\n",
    "random.seed(1234)\n",
    "abdo_fnames_to_use = random.sample(abdo_fnames, k=400)\n",
    "hn_fnames_to_use = random.sample(hn_fnames, k=400)\n",
    "\n",
    "# load the images\n",
    "train_ct_slices = np.zeros((400+400, 256, 256), dtype=float)\n",
    "# abdomen first\n",
    "for adx, fname in enumerate(abdo_fnames_to_use):\n",
    "    train_ct_slices[adx] = np.load(join(abdo_datapath, fname))\n",
    "# now the head and neck\n",
    "for hdx, fname in enumerate(hn_fnames_to_use):\n",
    "    im = np.load(join(hn_datapath, fname))\n",
    "    padded_im = np.pad(im, pad_width=32, constant_values=-1024)\n",
    "    train_ct_slices[hdx+400] = padded_im\n",
    "\n",
    "# create a labels array (array 0s - abdomenal ct, 1s = HnN ct)\n",
    "ground_truth_labels = np.zeros((800))\n",
    "ground_truth_labels[400:] = 1\n",
    "\n",
    "# shuffle the data!\n",
    "np.random.seed(1234)\n",
    "shuffle_indices = np.random.permutation(800)\n",
    "train_ct_slices = train_ct_slices[shuffle_indices]\n",
    "ground_truth_labels = ground_truth_labels[shuffle_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick sanity check**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "n = 47\n",
    "print(ground_truth_labels[n])\n",
    "ax.imshow(train_ct_slices[n], cmap='Greys_r')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the unlabelled test data - you will need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the test data\n",
    "try:\n",
    "    \n",
    "    IN_COLAB = True\n",
    "    !wget https://www.dropbox.com/s/9950rhyrs9kb8kj/classification_test_data.tar.gz?dl=0 -O /content/ClassificationProcessedData.tar\n",
    "    !tar -xf /content/ClassificationProcessedData.tar -C /content/\n",
    "    !rm /content/ClassificationProcessedData.tar\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    !wget https://www.dropbox.com/s/9950rhyrs9kb8kj/classification_test_data.tar.gz?dl=0 -O ./ClassificationProcessedData.tar\n",
    "    !tar -xf ./ClassificationProcessedData.tar -C ./\n",
    "    !rm ClassificationProcessedData.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your work starts here!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what pre-processing to apply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dataset class, **split into train/validation set**, think about augmentations you should use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using a simple classification model from [torchvision](https://pytorch.org/vision/stable/models.html) (e.g. ResNet18). Think about what loss and optimiser to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchvision should be imported, load a model with torchvision.models.MODEL_NAME\n",
    "## weights = 'IMAGENET1K_V1' tells torchvision to load the model with weights trained on IMAGENET\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to write your own training loops in Pytorch as you'll get a real understanding of what is happening \"under-the-hood\". \n",
    "\n",
    "_You can always use `pytorch-lightning` if you don't want to..._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model for a few epochs. Remember to monitor both the training and validation losses!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify & segment the testing images\n",
    "\n",
    "1. Run the classifier on each image in your dataset.\n",
    "2. Based on the output from step 1, select the appropriate segmentation model.\n",
    "3. Segment the images using the saved models from Part 1 & 2.\n",
    "4. Measure segmentation performance.\n",
    "   - What other results can you extract? Does segmentation performance depend on structure size? Is segmentation performance better or worse in head and neck? Why might this be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the segmentation models from part 1 & 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningFPN(pl.LightningModule):\n",
    "  def __init__(self, structure_names):\n",
    "    super().__init__()\n",
    "    ## Create the pytorch model \n",
    "    self.model = smp.FPN(\"resnet18\", in_channels=1, classes=len(structure_names)+1, encoder_weights='imagenet')\n",
    "    \n",
    "    ## Construct a loss function, this is DSC, configured for multiple classes, and ignoring the background\n",
    "    self.loss_fcn = smp.losses.DiceLoss(\"multiclass\", from_logits=True)\n",
    "\n",
    "    ## Specify which optimiser to use here\n",
    "    self.optimizer = torch.optim.Adam\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = self.optimizer(self.parameters(), lr=1e-4)## May need to handle other kwargs here!\n",
    "    return {\"optimizer\": optimizer, \"reduce_on_plateau\":True}\n",
    "    ## Note - we are reducing the learning rate when the validation loss plateaus for a while - this should improve the model\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    # Separate batch into input, mask and spacing (although we ignore the spacing)\n",
    "    img, msk, _ = batch\n",
    "    # Pass the input through the model and get a prediction (msk_hat)\n",
    "    msk_hat = self(img)\n",
    "    # Calculate average prediction error on this batch\n",
    "    loss = self.loss_fcn(msk_hat, msk.long())\n",
    "    # Log the error \n",
    "    self.log(\"loss\", loss)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    # Identical to the training step\n",
    "    img, msk, _ = batch\n",
    "    msk_hat = self(img)\n",
    "    val_loss = self.loss_fcn(msk_hat, msk.long())\n",
    "    self.log(\"val_loss\", val_loss)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the segmentation models from part 1 & 2\n",
    "# !! You'll need to update PATH_TO_WEIGHTS with the appropriate path \n",
    "# Abdominal model\n",
    "\n",
    "abdominal_model = LightningFPN(\n",
    "    ['Body', 'Liver', 'Kidneys', 'Spleen', 'Pancreas']).load_from_checkpoint(PATH_TO_WEIGHTS)\n",
    "\n",
    "# Head and neck (hnn) model\n",
    "hnn_model = LightningFPN(\n",
    "    ['Body', 'Brainstem', 'Mandible', 'Parotids', 'Spinalcord']).load_from_checkpoint(PATH_TO_WEIGHTS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Inference loop, iterate over your dataset and use classifier to predict anatomical location\n",
    "## Remember to apply an activation to your model predictions\n",
    "\n",
    "    ## 2 + 3. Based on the prediction, pass the image to the correct segmentation model\n",
    "\n",
    "    ## 4. Measure performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
